
# **Llama-Triton** 


> **Llama-Triton** æ˜¯å°‡ `llama.cpp` æ ¸å¿ƒä»¥ **Python + Triton** é‡å¯«çš„å°ˆæ¡ˆï¼Œæ”¯æ´ FP16 æ¨ç†ã€Page Attentionã€Rotary Embedding èˆ‡å¤š GPU åˆ†å±¤ï¼›å®¹æ˜“ hackã€æ˜“æ–¼å­¸ç¿’ï¼Œé©åˆæƒ³æ·±å…¥ç ”ç©¶ LLM æ¨ç†æ ¸å¿ƒæˆ–å®¢è£½åŒ– GPU kernels çš„é–‹ç™¼è€…ã€‚

---

## ç›®éŒ„

- [åŠŸèƒ½èªªæ˜](#åŠŸèƒ½èªªæ˜)
- [å¿«é€Ÿå®‰è£](#å¿«é€Ÿå®‰è£)
- [åŸºæœ¬ä½¿ç”¨](#åŸºæœ¬ä½¿ç”¨)
  - [CLI](#cli)
  - [Python API](#python-api)


---

## åŠŸèƒ½èªªæ˜

| åŠŸèƒ½ | èªªæ˜ |
|---------|------|
| **Triton Kernels** | GEMMã€Page Attentionã€RMSNorm å…¨ä»¥ Triton æ’°å¯«ï¼Œä¸€éµè‡ªå‹•èª¿å„ªã€‚ |
| **Page Attention** | ä»¥ã€Œé ã€ç‚ºå–®ä½ç®¡ç† KV-Cacheï¼Œé•·åºåˆ—æ¨ç†é¡¯è‘—ç¯€çœ GPU è¨˜æ†¶é«”ã€‚ |
| **æ¥µç°¡ä¾è³´** | åªéœ€ `torch >= 2.3` èˆ‡ `triton == 2.2`ã€‚ç„¡éœ€ç·¨è­¯ C++ã€‚ |
| **å¤š GPU åˆ†å±¤** | `devices=["cuda:0","cuda:1"]` å³å¯æŠŠå‰ N å±¤åˆ‡åˆ°å…¶å®ƒ GPUã€‚ |
| **GGUF è½‰æ›è…³æœ¬** | `scripts/convert_gguf.py` ä¸€éµæŠŠ llama.cpp æ¬Šé‡è½‰æˆ PyTorch `state_dict.pt`ã€‚ |
| **å®Œæ•´æ¸¬è©¦è¦†è“‹** | pytest è¦†è“‹ç‡ > 90%ï¼ŒCI free on GitHub Actionsã€‚ |


## å¿«é€Ÿå®‰è£

```bash
# å»ºè­° Python >= 3.10ï¼ŒCUDA 12.x
conda create -n llama-triton python=3.10 -y
conda activate llama-triton

pip install -r requirements.txt
```

> **è‹¥åƒ…ä½¿ç”¨ CPU** å¯åŠ ä¸Š `--extra-index-url https://download.pytorch.org/whl/cpu`ã€‚ä¸éç›®å‰ Triton kernel éœ€ CUDA ğŸ‘‰ CPU fallback é–‹ç™¼ä¸­ã€‚

## åŸºæœ¬ä½¿ç”¨

### CLI

```bash
python main.py ./models/llama2-7b \
  --prompt "å°ç£çš„é¦–éƒ½æ˜¯ï¼Ÿ" \
  --max_tokens 16
```

çµ‚ç«¯è¼¸å‡ºï¼š

```text
â€º å°åŒ—å¸‚ã€‚
```

### Python API

```python
from api.inference import LlamaGenerator

gen = LlamaGenerator("./models/llama2-7b", device="cuda")
print(gen.generate("The capital of France is", max_new_tokens=16))
```

> äº¦å¯ä½¿ç”¨ `gen.generate_batch([...])` é€²è¡Œæ‰¹æ¬¡æ¨ç†ã€‚



